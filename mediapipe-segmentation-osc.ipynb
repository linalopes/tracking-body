{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "This notebook, created by Lina Lopes in December 2024, demonstrates the use of a pre-trained machine learning model for real-time body segmentation (Media Pipe). The goal is to identify the silhouette of a body and transmit the output to a Syphon server, enabling dynamic masking for projection mapping in videomapping software.\n",
    "\n",
    "The segmentation leverages MediaPipe's Selfie Segmentation model to process live video feeds, isolating the body region and creating a mask. This mask can be used creatively as a dynamic layer for visual projections, allowing for immersive and interactive installations.\n",
    "\n",
    "For more projects, visit [Lina Lopes on GitHub](https://github.com/LinaLopes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Purpose: This cell imports the necessary libraries:\n",
    "\n",
    "- cv2: OpenCV for video capture and manipulation.\n",
    "- mediapipe: Google's framework for segmentation and other AI tasks.\n",
    "- numpy: Numerical operations.\n",
    "- Syphon: For video stream sharing.\n",
    "- glfw: For managing windows and OpenGL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import Syphon\n",
    "import glfw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Configuration\n",
    "\n",
    "1. Initializes MediaPipe Selfie Segmentation for real-time body segmentation with model_selection=1 for high-quality results.\n",
    "2. Defines the output window size for streaming: (1280, 720).\n",
    "3. Sets up a Syphon server named \"Syphon Body Silhouette\" for transmitting video frames to external applications like MadMapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733336662.852597 2264590 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733336662.869466 2264942 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-12-04 19:24:22.984 python[34492:2264590] SYPHON DEBUG: SyphonServerConnectionManager: Start Connection\n",
      "2024-12-04 19:24:22.984 python[34492:2264590] SYPHON DEBUG: SyphonServerConnectionManager: Created connection with UUID: info.v002.Syphon.20C83AC0-9593-42B5-9D63-C0975634B762\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Selfie Segmentation\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=1)\n",
    "\n",
    "# window details\n",
    "size = (1280, 720)\n",
    "\n",
    "# Configure o servidor Syphon\n",
    "server = Syphon.Server(\"Syphon Body Silhouette\", size, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Test/Example\n",
    "\n",
    "1. Capture Input: Initializes the video capture (update the index 3 to match your device).\n",
    "2. Mirroring (Optional): Flips the frame for a mirrored effect.\n",
    "3. Preprocessing: Converts frames from BGR to RGB (MediaPipe's input requirement).\n",
    "4. Segmentation Mask: Processes the frame with MediaPipe, generating a binary mask to isolate the body.\n",
    "5. Blurring: Softens mask edges using Gaussian blur for smoother segmentation.\n",
    "6. Body Isolation: Combines the mask and frame to display only the segmented body region.\n",
    "7. Display: Shows the segmented output in a window titled \"Body Segmentation\".\n",
    "8. Exit Condition: Stops processing when the 'q' key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start video capture\n",
    "cap = cv2.VideoCapture(3)  # Change 0 to the desired camera index\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirrored view (optional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert BGR frame to RGB as MediaPipe requires RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to get segmentation results\n",
    "    results = selfie_segmentation.process(rgb_frame)\n",
    "\n",
    "    # Create a mask where the detected body is white (255) and the background is black (0)\n",
    "    body_mask = (results.segmentation_mask > 0.6).astype(np.uint8) * 255 # Replace 0.5 with a larger value, such as 0.6 or 0.7, to include only areas with higher confidence.\n",
    "\n",
    "    # Use the cv2.GaussianBlur or cv2.blur filter to soften the edge. This softens the binary mask, creating a more gradual transition.\n",
    "    body_mask = cv2.GaussianBlur(body_mask, (15, 15), 0)\n",
    "\n",
    "    # Extract only the body region from the original frame\n",
    "    body_only = cv2.bitwise_and(frame, frame, mask=body_mask)\n",
    "\n",
    "    # Display the segmented body region\n",
    "    cv2.imshow(\"Body Segmentation\", body_only)\n",
    "\n",
    "    # Exit loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To send by Syphon to MadMapper\n",
    "\n",
    "1. Segmentation Process - Confidence Threshold\n",
    "    ```\n",
    "    body_mask = (results.segmentation_mask > 0.6).astype(np.uint8) * 255\n",
    "    ```\n",
    "\n",
    "    The 0.6 threshold determines how confident the model must be to classify a pixel as part of the body. Higher thresholds (e.g., 0.7) make the segmentation more selective, reducing false positives but potentially missing some body parts. Lower thresholds (e.g., 0.5) make it more inclusive but may include background noise. Students can experiment with this value to balance segmentation quality.\n",
    "\n",
    "2. Gaussian Blur\n",
    "    ```\n",
    "    body_mask = cv2.GaussianBlur(body_mask, (15, 15), 0)\n",
    "    ```\n",
    "\n",
    "    The Gaussian blur smooths the mask edges, reducing sharp transitions and artifacts. This is particularly useful when displaying the segmented region, making it visually pleasing and avoiding jagged edges. The kernel size (15, 15) can be adjusted to increase or decrease the level of blurring.\n",
    "\n",
    "3. Syphon Integration\n",
    "    ```\n",
    "    server.publish_frame(body_only)\n",
    "    ```\n",
    "\n",
    "    This line sends the processed frame (segmented body) to the Syphon server. Tools like MadMapper can then receive and display this output in real time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 19:24:31.873 python[34492:2264590] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "# Start video capture\n",
    "cap = cv2.VideoCapture(3)  # Change 3 to the desired camera index\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for a mirrored view (optional)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert BGR frame to RGB as required by MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame to get segmentation results\n",
    "    results = selfie_segmentation.process(rgb_frame)\n",
    "\n",
    "    # Create a mask where the detected body is white (255) and the background is black (0)\n",
    "    body_mask = (results.segmentation_mask > 0.6).astype(np.uint8) * 255\n",
    "\n",
    "    # Create a blank image (black background) and apply the mask as white\n",
    "    silhouette = np.zeros_like(frame, dtype=np.uint8)  # Black background\n",
    "    silhouette[body_mask == 255] = [255, 255, 255]  # White silhouette\n",
    "\n",
    "    # Resize the silhouette to match the server size\n",
    "    silhouette_resized = cv2.resize(silhouette, size)\n",
    "\n",
    "    # Convert the silhouette to RGB format as required by Syphon\n",
    "    silhouette_rgb = cv2.cvtColor(silhouette_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Send the silhouette to the Syphon server\n",
    "    server.draw_and_send(silhouette_rgb)\n",
    "\n",
    "    # Display the silhouette locally (optional)\n",
    "    cv2.imshow(\"Body Silhouette\", silhouette)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    key = cv2.waitKey(1) & 0xFF  # Read key press\n",
    "    if key == ord('q'):  # Check if 'q' is pressed\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
